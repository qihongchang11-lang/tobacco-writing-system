"""
æ™ºèƒ½æ ·æœ¬æ£€ç´¢ç³»ç»Ÿ - BM25 + è¯­ä¹‰ç›¸ä¼¼åº¦æ··åˆæ£€ç´¢
ç”¨äºå­¦ä¹ é©±åŠ¨çš„æ”¹å†™ç³»ç»Ÿï¼Œä»æ ·æœ¬åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡ç« 
"""

import json
import math
import re
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntelligentRetriever:
    """æ™ºèƒ½æ ·æœ¬æ£€ç´¢å™¨"""

    def __init__(self, data_file: str = "data/samples/structured_articles.json"):
        self.data_file = Path(data_file)
        self.articles = []
        self.bm25_index = {}
        self.vocab = set()
        self.idf_scores = {}

        # åŠ è½½æ•°æ®
        self._load_articles()
        self._build_bm25_index()

    def _load_articles(self) -> None:
        """åŠ è½½ç»“æ„åŒ–æ–‡ç« æ•°æ®"""
        try:
            if not self.data_file.exists():
                logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
                return

            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.articles = data.get('articles', [])

            logger.info(f"åŠ è½½äº† {len(self.articles)} ç¯‡æ–‡ç« ")

        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ç« æ•°æ®å¤±è´¥: {e}")

    def _tokenize(self, text: str) -> List[str]:
        """ä¸­æ–‡æ–‡æœ¬åˆ†è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', ' ', text)
        # åˆ†å‰²ä¸ºå­—ç¬¦ï¼ˆä¸­æ–‡æŒ‰å­—åˆ†è¯ï¼‰
        tokens = []
        for char in text:
            if char.strip() and char not in [' ', '\t', '\n']:
                tokens.append(char)

        # ç»„åˆå¸¸è§è¯æ±‡
        text_clean = ''.join(tokens)
        common_words = [
            'çƒŸè‰', 'å·çƒŸ', 'çƒŸå¶', 'ä¸“å–', 'è¥é”€', 'é”€å”®', 'ç›‘ç®¡', 'ç”Ÿäº§',
            'ä¼šè®®', 'å¬å¼€', 'ä¸¾åŠ', 'æ´»åŠ¨', 'å·¥ä½œ', 'å‘å±•', 'å»ºè®¾', 'ç®¡ç†',
            'åŒæ¯”', 'å¢é•¿', 'ä¸‹é™', 'æå‡', 'ä¼˜åŒ–', 'æ¨è¿›', 'è½å®', 'éƒ¨ç½²'
        ]

        for word in common_words:
            if word in text_clean:
                tokens.append(word)

        return tokens

    def _build_bm25_index(self) -> None:
        """æ„å»ºBM25ç´¢å¼•"""
        if not self.articles:
            return

        # è®¡ç®—æ–‡æ¡£è¯é¢‘
        doc_tokens = []
        all_tokens = []

        for article in self.articles:
            # åˆå¹¶æ ‡é¢˜ã€å¯¼è¯­ã€æ­£æ–‡
            full_text = f"{article.get('title', '')} {article.get('lead', '')} {article.get('body', '')}"
            tokens = self._tokenize(full_text)
            doc_tokens.append(tokens)
            all_tokens.extend(tokens)

        # å»ºç«‹è¯æ±‡è¡¨
        self.vocab = set(all_tokens)

        # è®¡ç®—IDF
        doc_count = len(self.articles)
        for token in self.vocab:
            doc_freq = sum(1 for tokens in doc_tokens if token in tokens)
            self.idf_scores[token] = math.log((doc_count - doc_freq + 0.5) / (doc_freq + 0.5))

        # æ„å»ºç´¢å¼•
        for i, tokens in enumerate(doc_tokens):
            token_counts = Counter(tokens)
            self.bm25_index[i] = {
                'tokens': token_counts,
                'length': len(tokens)
            }

        logger.info(f"æ„å»ºBM25ç´¢å¼•å®Œæˆï¼Œè¯æ±‡é‡: {len(self.vocab)}")

    def _calculate_bm25_score(self, query_tokens: List[str], doc_index: int) -> float:
        """è®¡ç®—BM25ç›¸ä¼¼åº¦å¾—åˆ†"""
        if doc_index not in self.bm25_index:
            return 0.0

        doc_info = self.bm25_index[doc_index]
        doc_tokens = doc_info['tokens']
        doc_length = doc_info['length']

        # BM25å‚æ•°
        k1, b = 1.2, 0.75
        avg_doc_length = sum(info['length'] for info in self.bm25_index.values()) / len(self.bm25_index)

        score = 0.0
        for token in query_tokens:
            if token not in self.vocab:
                continue

            tf = doc_tokens.get(token, 0)
            idf = self.idf_scores.get(token, 0)

            # BM25å…¬å¼
            numerator = tf * (k1 + 1)
            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))
            score += idf * (numerator / denominator)

        return score

    def _calculate_semantic_similarity(self, query: str, article: Dict[str, Any]) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åˆå¹¶æ–‡ç« æ–‡æœ¬
        article_text = f"{article.get('title', '')} {article.get('lead', '')} {article.get('body', '')}"

        # å…³é”®è¯åŒ¹é…
        query_lower = query.lower()
        article_lower = article_text.lower()

        # æ ¸å¿ƒæ¦‚å¿µåŒ¹é…
        tobacco_concepts = ['çƒŸè‰', 'å·çƒŸ', 'çƒŸå¶', 'ä¸“å–', 'è¥é”€', 'é”€å”®', 'ç›‘ç®¡']
        business_concepts = ['ä¼šè®®', 'æ´»åŠ¨', 'å·¥ä½œ', 'å‘å±•', 'å»ºè®¾', 'ç®¡ç†', 'éƒ¨ç½²']
        data_concepts = ['å¢é•¿', 'ä¸‹é™', 'åŒæ¯”', 'ç¯æ¯”', 'æ•°æ®', 'ç»Ÿè®¡', 'åˆ†æ']

        concept_groups = [tobacco_concepts, business_concepts, data_concepts]

        similarity = 0.0

        # æ¦‚å¿µç»„åŒ¹é…
        for concepts in concept_groups:
            query_match = sum(1 for concept in concepts if concept in query_lower)
            article_match = sum(1 for concept in concepts if concept in article_lower)
            if query_match > 0 and article_match > 0:
                similarity += min(query_match, article_match) / len(concepts)

        # æ ç›®åŒ¹é…å¥–åŠ±
        features = article.get('features', {})
        column_indicators = features.get('column_indicators', {})

        column_keywords = {
            'news_general': ['ä¼šè®®', 'å¬å¼€', 'ä¸¾åŠ', 'æ´»åŠ¨'],
            'economic_data': ['å¢é•¿', 'é”€å”®', 'æ”¶å…¥', 'æ•°æ®', '%'],
            'policy_interpretation': ['æ”¿ç­–', 'é€šçŸ¥', 'å…¬å‘Š', 'è§„å®š'],
            'case_observation': ['å…¸å‹', 'å…ˆè¿›', 'æ¡ˆä¾‹', 'ç»éªŒ']
        }

        for column, keywords in column_keywords.items():
            if any(kw in query_lower for kw in keywords) and column_indicators.get(column, False):
                similarity += 0.3

        return min(similarity, 1.0)

    def retrieve_similar_samples(
        self,
        query_text: str,
        target_column: Optional[str] = None,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸ä¼¼æ ·æœ¬

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            target_column: ç›®æ ‡æ ç›®ï¼ˆå¯é€‰è¿‡æ»¤ï¼‰
            top_k: è¿”å›æ•°é‡

        Returns:
            ç›¸ä¼¼æ–‡ç« åˆ—è¡¨ï¼ŒåŒ…å«ç›¸ä¼¼åº¦å¾—åˆ†
        """
        if not self.articles:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„æ–‡ç« æ•°æ®")
            return []

        query_tokens = self._tokenize(query_text)

        candidates = []

        for i, article in enumerate(self.articles):
            # æ ç›®è¿‡æ»¤
            if target_column:
                features = article.get('features', {})
                column_indicators = features.get('column_indicators', {})
                if not column_indicators.get(target_column, False):
                    continue

            # è®¡ç®—BM25å¾—åˆ†
            bm25_score = self._calculate_bm25_score(query_tokens, i)

            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_score = self._calculate_semantic_similarity(query_text, article)

            # æ··åˆå¾—åˆ† (BM25: 40%, è¯­ä¹‰: 60%)
            combined_score = 0.4 * bm25_score + 0.6 * semantic_score

            candidates.append({
                'article': article,
                'bm25_score': bm25_score,
                'semantic_score': semantic_score,
                'combined_score': combined_score
            })

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        candidates.sort(key=lambda x: x['combined_score'], reverse=True)

        # å¤šæ ·æ€§æ§åˆ¶ï¼šé¿å…ç›¸ä¼¼æ–‡ç« èšé›†
        diverse_results = []
        used_titles = set()

        for candidate in candidates:
            if len(diverse_results) >= top_k:
                break

            title = candidate['article'].get('title', '')
            # ç®€å•å»é‡ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦è¿‡é«˜çš„è·³è¿‡
            is_duplicate = any(
                self._title_similarity(title, used_title) > 0.7
                for used_title in used_titles
            )

            if not is_duplicate:
                diverse_results.append({
                    'article_id': candidate['article']['id'],
                    'title': title,
                    'lead': candidate['article'].get('lead', ''),
                    'body': candidate['article'].get('body', ''),
                    'similarity_score': round(candidate['combined_score'], 3),
                    'features': candidate['article'].get('features', {}),
                    'full_article': candidate['article']
                })
                used_titles.add(title)

        logger.info(f"æ£€ç´¢åˆ° {len(diverse_results)} ä¸ªç›¸å…³æ ·æœ¬ (æŸ¥è¯¢: {query_text[:20]}...)")
        return diverse_results

    def _title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
        if not title1 or not title2:
            return 0.0

        # ç®€å•çš„å­—ç¬¦é‡å ç‡
        set1 = set(title1)
        set2 = set(title2)
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯"""
        if not self.articles:
            return {"total_articles": 0, "vocab_size": 0}

        # æ ç›®åˆ†å¸ƒ
        column_distribution = {}
        for article in self.articles:
            features = article.get('features', {})
            column_indicators = features.get('column_indicators', {})
            for column, has_indicator in column_indicators.items():
                if has_indicator:
                    column_distribution[column] = column_distribution.get(column, 0) + 1

        return {
            "total_articles": len(self.articles),
            "vocab_size": len(self.vocab),
            "column_distribution": column_distribution,
            "avg_article_length": sum(len(article.get('body', '')) for article in self.articles) / len(self.articles),
            "index_status": "ready" if self.bm25_index else "not_built"
        }


def main():
    """æµ‹è¯•å‡½æ•°"""
    retriever = IntelligentRetriever()

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = retriever.get_statistics()
    print(f"ğŸ“Š æ£€ç´¢å™¨ç»Ÿè®¡: {stats}")

    # æµ‹è¯•æ£€ç´¢
    test_queries = [
        "å±±ä¸œçœçƒŸè‰å¬å¼€ä¼šè®®æ¨è¿›è¥é”€å·¥ä½œ",
        "é”€å”®æ”¶å…¥å¢é•¿æ•°æ®åˆ†æ",
        "æ”¿ç­–è§£è¯»é€šçŸ¥å‘å¸ƒ"
    ]

    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = retriever.retrieve_similar_samples(query, top_k=2)
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['title']} (ç›¸ä¼¼åº¦: {result['similarity_score']})")


if __name__ == "__main__":
    main()